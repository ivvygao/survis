@misc{feng2023diverse,
  archiveprefix = {arXiv},
  author = {Chun-Mei Feng and Kai Yu and Yong Liu and Salman Khan and Wangmeng Zuo},
  eprint = {2308.06038},
  primaryclass = {cs.CV},
  title = {Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning},
  keywords = {Prompt learning, Vision-Language Models, Data Augmentation},
  year = {2023}
}

@misc{karmanov2024efficient,
  archiveprefix = {arXiv},
  author = {Adilbek Karmanov and Dayan Guan and Shijian Lu and Abdulmotaleb El Saddik and Eric Xing},
  eprint = {2403.18293},
  primaryclass = {cs.CV},
  title = {Efficient Test-Time Adaptation of Vision-Language Models},
  keywords = {Prompt learning, Vision-Language Models},
  year = {2024}
}

@misc{shu2022testtime,
  archiveprefix = {arXiv},
  author = {Manli Shu and Weili Nie and De-An Huang and Zhiding Yu and Tom Goldstein and Anima Anandkumar and Chaowei Xiao},
  eprint = {2209.07511},
  primaryclass = {cs.CV},
  title = {Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models},
  year = {2022},
  keywords = {Prompt learning,domain adaptation, Vision-Language Models}
}

@article{Zhao2024,
  abstract = {Prompt learning stands out as one of the most efficient approaches for adapting powerful vision-language foundational models like CLIP to downstream datasets by tuning learnable prompt vectors with very few samples. However, despite its success in achieving remarkable performance on in-domain data, prompt learning still faces the significant challenge of effectively generalizing to novel classes and domains. Some existing methods address this concern by dynamically generating distinct prompts for different domains. Yet, they overlook the inherent potential of prompts to generalize across unseen domains. To address these limitations, our study introduces an innovative prompt learning paradigm, called MetaPrompt, aiming to directly learn domain invariant prompt in few-shot scenarios. To facilitate learning prompts for image and text inputs independently, we present a dual-modality prompt tuning network comprising two pairs of coupled encoders. Our study centers on an alternate episodic training algorithm to enrich the generalization capacity of the learned prompts. In contrast to traditional episodic training algorithms, our approach incorporates both in-domain updates and domain-split updates in a batch-wise manner. For in-domain updates, we introduce a novel asymmetric contrastive learning paradigm, where representations from the pre-trained encoder assume supervision to regularize prompts from the prompted encoder. To enhance performance on out-of-domain distribution, we propose a domain-split optimization on visual prompts for cross-domain tasks or textual prompts for cross-class tasks during domain-split updates. Extensive experiments across 11 datasets for base-to-new generalization and 4 datasets for domain generalization exhibit favorable performance. Compared with the state-of-the-art method, MetaPrompt achieves an absolute gain of 1.02% on the overall harmonic mean in base-to-new generalization and consistently demonstrates superiority over all benchmarks in domain generalization.},
  author = {Cairong Zhao and Yubin Wang and Xinyang Jiang and Yifei Shen and Kaitao Song and Dongsheng Li and Duoqian Miao},
  doi = {10.1109/TIP.2024.3362062},
  issn = {19410042},
  journal = {IEEE Transactions on Image Processing},
  keywords = {Prompt learning,domain generalization,few-shot learning,meta-learning, Vision-Language Models},
  pages = {1348-1360},
  pmid = {38335087},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title = {Learning Domain Invariant Prompt for Vision-Language Models},
  volume = {33},
  year = {2024}
}
@article{Tian2024,
   abstract = {Recently, the significant success of the large pre-trained models have attracted great attentions. How to sufficiently use these models is a big issue. Black-box domain adaptation is a way which tries to train a target model by a cloud API offered by a large pre-trained model without model details and source data. The existing black-box domain adaptation methods for image classification always use the prediction results from the cloud API, but the information is very limited. On the other hand, the recent proposed visual-language model (CLIP), trained from a large number of extensive datasets, aligns the visual feature and text feature in a common space, which provides useful auxiliary information. In this work, we propose a new black-box domain adaptation method guided by CLIP (BBC). The key idea is to generate more accurate pseudo-labels. Two strategies are adapted. The first is called generation of joint pseudo-labels, which combines the predictions from cloud API and CLIP model. Another one is the structure-preserved pseudo-labeling strategy which further generates much better pseudo-labels by the previous stored predictions of the k-closest neighbors. Experiments on three benchmark datasets show that our method achieves the state-of-the-art results with large margin.},
   author = {Liang Tian and Mao Ye and Lihua Zhou and Qichen He},
   doi = {10.1007/s11760-024-03101-8},
   issn = {18631711},
   journal = {Signal, Image and Video Processing},
   keywords = {Black-box domain adaptation,Image classification,Vision-language models},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {CLIP-guided black-box domain adaptation of image classification},
   year = {2024},
}
@misc{zhang2023domainadaptor,
      title={DomainAdaptor: A Novel Approach to Test-time Adaptation}, 
      author={Jian Zhang and Lei Qi and Yinghuan Shi and Yang Gao},
      year={2023},
      eprint={2308.10297},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      keywords={Test-Time Adaptation, domain shift, entropy minimization}
}

@misc{wang2021tent,
      title={Tent: Fully Test-time Adaptation by Entropy Minimization}, 
      author={Dequan Wang and Evan Shelhamer and Shaoteng Liu and Bruno Olshausen and Trevor Darrell},
      year={2021},
      eprint={2006.10726},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      keywords={Test-Time Adaptation, Feature Modulation, Entropy Minimization}
}

@misc{boudiaf2022parameterfree,
      title={Parameter-free Online Test-time Adaptation}, 
      author={Malik Boudiaf and Romain Mueller and Ismail Ben Ayed and Luca Bertinetto},
      year={2022},
      eprint={2201.05718},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      keywords={Test-Time Adaptation, Domain-Invariant Prompting, Few-Shot Learning}
}

@misc{huang2022unsupervised,
      title={Unsupervised Prompt Learning for Vision-Language Models}, 
      author={Tony Huang and Jack Chu and Fangyun Wei},
      year={2022},
      eprint={2204.03649},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      keywords = {Pre-training model,Prompt,Vision-language models}
}

@misc{zhu2024promptaligned,
      title={Prompt-aligned Gradient for Prompt Tuning}, 
      author={Beier Zhu and Yulei Niu and Yucheng Han and Yue Wu and Hanwang Zhang},
      year={2024},
      eprint={2205.14865},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      keywords = {Pre-training model,Prompt,Vision-language models}
}

@article{Xing2024,
   abstract = {Vision Language Model (VLM) is a popular research field located at the fusion of computer vision and natural language processing (NLP). With the emergence of transformer networks and mass web data, numerous large scale VLMs or Vision-Language Pre-training Models (VLPM) have been achieving state-of-the-art results in many tasks, such as retrieval (CLIP) and generation (DALL-E). Although large models have shown impressive results, the cost of retraining and full fine-tuning is prohibitive for general researchers. In recent years, Efficient fine-tuning (EFT) which a very low-cost tuning method has been a good solution to this problem has greatly alleviated this problem, and driven by this, a new fine-tuning paradigm has developed. Since Prompt and Adapter are most widely used in the field of visual language, this review focuses on analysing the progress of the application of these two methods. Firstly, we reviewed the VLM research paradigm based on the differences in pre-training-fine-tuning methods; Next, We categorized the Prompt into 3 types (7 subtypes) of usage patterns based on the different modal information, and categorized the Adapter into 2 types of usage patterns based on whether it plays a role in modal fusion, furthermore we discussed them in vision and vision-language tasks. Finally, we discussed the stability and social ethics of EFT, and possible future research directions were proposed.},
   author = {Jialu Xing and Jianping Liu and Jian Wang and Lulu Sun and Xi Chen and Xunxun Gu and Yingfei Wang},
   doi = {10.1016/j.cag.2024.01.012},
   issn = {00978493},
   journal = {Computers and Graphics (Pergamon)},
   keywords = {Adapter,Computer vision,Efficient fine-tuning,Pre-training model,Prompt,Vision-language models},
   month = {4},
   publisher = {Elsevier Ltd},
   title = {A survey of efficient fine-tuning methods for Vision-Language Models â€” Prompt and Adapter},
   volume = {119},
   year = {2024},
}
@article{Zhang2024,
   abstract = {Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlation from web-scale image-text pairs that are almost infinitely available on the Internet and enables zero-shot predictions on various visual recognition tasks with a single VLM. This paper provides a systematic review of visual language models for various visual recognition tasks, including: (1) the background that introduces the development of visual recognition paradigms; (2) the foundations of VLM that summarize the widely-adopted network architectures, pre-training objectives, and downstream tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4) the review and categorization of existing VLM pre-training methods, VLM transfer learning methods, and VLM knowledge distillation methods; (5) the benchmarking, analysis and discussion of the reviewed methods; (6) several research challenges and potential research directions that could be pursued in the future VLM studies for visual recognition. A project associated with this survey has been created at https://github.com/jingyi0000/VLM_survey.},
   author = {Jingyi Zhang and Jiaxing Huang and Sheng Jin and Shijian Lu},
   doi = {10.1109/TPAMI.2024.3369699},
   issn = {19393539},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Data models,Deep learning,Predictive models,Surveys,Task analysis,Training,Visual recognition,Visualization,big data,big model,deep learning,deep neural network,image classification,knowledge distillation,object detection,pre-training,semantic segmentation,transfer learning,vision-language model},
   publisher = {IEEE Computer Society},
   title = {Vision-Language Models for Vision Tasks: A Survey},
   year = {2024},
}



